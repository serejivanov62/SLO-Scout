# ğŸ¯ SLO-Scout: ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ SRE

## ĞĞ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
1. [ĞĞ±Ñ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ](#Ğ¾Ğ±Ñ‰Ğ°Ñ-ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ)
2. [ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹](#Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹)
3. [ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ](#ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹-Ğ¸-Ğ¸Ñ…-Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ)
4. [ĞŸĞ¾Ñ‚Ğ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…](#Ğ¿Ğ¾Ñ‚Ğ¾Ğº-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
5. [Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°](#Ğ±Ğ°Ğ·Ğ°-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…-Ğ¸-Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°)
6. [Deployment Ğ¸ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°](#deployment-Ğ¸-Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°)
7. [ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ](#Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³-Ğ¸-Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ)
8. [ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ](#Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ-Ğ¸-Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ)
9. [Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ](#Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ)
10. [ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹](#Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ-Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹)

---

## ĞĞ±Ñ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ

### Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ SLO-Scout?

**SLO-Scout** â€” ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Service Level Indicators (SLI) Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Service Level Objectives (SLO) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµĞ»ĞµĞ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ observability ÑĞ¸ÑÑ‚ĞµĞ¼.

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ñ€ĞµÑˆĞ°ĞµÑ‚:

Ğ’ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑĞ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° SLO Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ:
- Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°
- ĞÑ‚Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ~2-4 Ñ‡Ğ°ÑĞ° Ğ½Ğ° ÑĞµÑ€Ğ²Ğ¸Ñ Ñƒ SRE ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹
- Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹
- Ğ§Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ user journeys

### Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ SLO-Scout:

1. **ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚** distributed traces, metrics, logs
2. **ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ user journeys** Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ graph traversal
3. **Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ SLI ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹** Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
4. **Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ production-ready Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹**: Prometheus rules, Grafana dashboards, runbooks
5. **Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ GitOps** Ñ‡ĞµÑ€ĞµĞ· PR Ğ² Git Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹

**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚**: Ğ’Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ SLO ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ñ 2-4 Ñ‡Ğ°ÑĞ¾Ğ² Ğ´Ğ¾ 15-30 Ğ¼Ğ¸Ğ½ÑƒÑ‚.

---

## ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Telemetry Sources                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Prometheus  â”‚  â”‚    OTLP     â”‚  â”‚  FluentBit  â”‚  â”‚  Kubernetes â”‚   â”‚
â”‚  â”‚   Metrics   â”‚  â”‚   Traces    â”‚  â”‚    Logs     â”‚  â”‚   Metrics   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                 â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Collectors Layer (Go)       â”‚
                    â”‚  â€¢ prometheus-collector (2x)   â”‚
                    â”‚  â€¢ otlp-collector (3x)         â”‚
                    â”‚  â€¢ log-collector (DaemonSet)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Apache Kafka 3.5+          â”‚
                    â”‚  Topics:                       â”‚
                    â”‚  â€¢ raw-telemetry               â”‚
                    â”‚  â€¢ enriched-events             â”‚
                    â”‚  â€¢ capsules                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fingerprint Job   â”‚  â”‚ Embedding Pipeline   â”‚  â”‚  Backend Service   â”‚
â”‚  (Apache Flink)    â”‚  â”‚  (Apache Flink)      â”‚  â”‚   (FastAPI)        â”‚
â”‚                    â”‚  â”‚                      â”‚  â”‚                    â”‚
â”‚ â€¢ Trace analysis   â”‚  â”‚ â€¢ Sentence Trans.    â”‚  â”‚ â€¢ REST API         â”‚
â”‚ â€¢ User journey     â”‚  â”‚ â€¢ Vector generation  â”‚  â”‚ â€¢ Analysis jobs    â”‚
â”‚   detection        â”‚  â”‚ â€¢ Milvus write       â”‚  â”‚ â€¢ SLI/SLO logic    â”‚
â”‚ â€¢ Capsule creation â”‚  â”‚                      â”‚  â”‚ â€¢ Artifact gen     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                         â”‚                         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Storage Layer             â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ PostgreSQL 14 (TimescaleDB) â”‚
                    â”‚   - Services, SLIs, SLOs       â”‚
                    â”‚   - Telemetry Events           â”‚
                    â”‚   - Capsules (user journeys)   â”‚
                    â”‚                                â”‚
                    â”‚ â€¢ Milvus 2.3+ (Vector DB)     â”‚
                    â”‚   - Journey embeddings         â”‚
                    â”‚   - Semantic search            â”‚
                    â”‚                                â”‚
                    â”‚ â€¢ MinIO / S3                   â”‚
                    â”‚   - Raw traces (cold storage)  â”‚
                    â”‚   - Generated artifacts        â”‚
                    â”‚                                â”‚
                    â”‚ â€¢ Redis 7.2+                   â”‚
                    â”‚   - Celery task queue          â”‚
                    â”‚   - Cache layer                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       Output Layer             â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ Prometheus Alert Rules       â”‚
                    â”‚ â€¢ Grafana Dashboards           â”‚
                    â”‚ â€¢ SLO YAML definitions         â”‚
                    â”‚ â€¢ Markdown Runbooks            â”‚
                    â”‚ â€¢ GitHub/GitLab PRs            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Topology

- **Kubernetes (K3s/K8s)** â€” Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
- **Helm Chart** â€” ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ deployment
- **Multi-AZ** â€” Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ (production)
- **Resource Requests/Limits** â€” predictable resource allocation

---

## ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ

### 1. Collectors Layer (Go 1.21+)

#### Prometheus Collector
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `collectors/prometheus-collector/`

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ¿Ñ€Ğ¾Ñ Prometheus remote_read API
- Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
- ĞŸÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ² Kafka topic `raw-telemetry`

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- ĞĞ°Ğ¿Ğ¸ÑĞ°Ğ½ Ğ½Ğ° Go Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ latency
- Hot-reload ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ConfigMap
- Reconciliation loop: 60 ÑĞµĞºÑƒĞ½Ğ´ (fallback Ğº polling)
- fsnotify watcher Ğ´Ğ»Ñ immediate updates

**Ğ ĞµÑÑƒÑ€ÑÑ‹**:
```yaml
requests:
  memory: 128Mi
  cpu: 100m
limits:
  memory: 512Mi
  cpu: 500m
```

**Healthchecks**: `/health` endpoint Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ñƒ 8080

---

#### OTLP Collector
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `collectors/otlp-collector/`

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- ĞŸÑ€Ğ¸ĞµĞ¼ OpenTelemetry traces/metrics Ñ‡ĞµÑ€ĞµĞ· gRPC (4317) Ğ¸ HTTP (4318)
- ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ OTLP protobuf Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°
- Enrichment: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ service metadata
- ĞŸÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ² Kafka topic `raw-telemetry`

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: 3-20 pods (HPA)
- Ğ¡Ñ‚Ğ°Ñ‚eless design Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ scaling
- Batch processing Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸
- Target: 70% CPU utilization Ğ´Ğ»Ñ autoscaling

**Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ SRE**:
- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ»Ñ trace ingestion
- ĞŸÑ€Ğ¸ ÑĞ±Ğ¾Ğµ traces Ñ‚ĞµÑ€ÑÑÑ‚ÑÑ (no disk buffering Ğ² collectors)
- HPA Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ Ñ traffic spikes

---

#### Log Collector
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `collectors/log-collector/main.go`

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- DaemonSet deployment Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ½Ğ¾Ğ´Ğµ
- Ğ¡Ğ±Ğ¾Ñ€ Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Kubernetes API Ğ¸Ğ»Ğ¸ tail Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
- Structured logging parsing (JSON)
- ĞŸÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ² Kafka topic `raw-telemetry`

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- DaemonSet ensures 1 pod per node
- Filesystem watching Ñ fsnotify
- Graceful shutdown Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ ConfigMap
- Reconciliation loop 60s (fallback)

---

### 2. Stream Processing Layer (Apache Flink 1.17)

#### Fingerprint Job
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `streaming/flink/src/main/java/fingerprinting/`

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- Real-time Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· distributed traces
- ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² (trace graph)
- ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ user journeys Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ path analysis
- Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ "capsules" â€” Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ journey

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹**:
1. **Stream from Kafka**: Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ `raw-telemetry` topic
2. **Keyed by trace_id**: Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ spans Ğ¿Ğ¾ trace
3. **Window**: tumbling window 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚
4. **Graph construction**: ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ DAG Ğ¸Ğ· spans
5. **Fingerprinting**: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ hash Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ:
   - ĞŸĞ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
   - HTTP methods
   - Status codes
6. **Capsule creation**: Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ traces
7. **Output to Kafka**: Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒĞµÑ‚ Ğ² `capsules` topic

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- State backend: RocksDB (checkpointing)
- Checkpointing interval: 30 ÑĞµĞºÑƒĞ½Ğ´
- Parallelism: 4 task slots
- Kryo serialization Ğ´Ğ»Ñ custom objects

**Ğ ĞµÑÑƒÑ€ÑÑ‹**:
```yaml
TaskManager:
  memory: 2Gi
  cpu: 1000m
  replicas: 2
```

**Ğ’Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ SRE**:
- Stateful application (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ persistent volume Ğ´Ğ»Ñ checkpoints)
- ĞŸÑ€Ğ¸ Ñ€ĞµÑÑ‚Ğ°Ñ€Ñ‚Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ checkpoint
- Backpressure monitoring Ñ‡ĞµÑ€ĞµĞ· Flink UI

---

#### Embedding Pipeline Job
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `streaming/embedding-pipeline-job/`

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ semantic embeddings Ğ´Ğ»Ñ capsules
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Sentence Transformers (all-MiniLM-L6-v2)
- Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ vectors Ğ² Milvus Ğ´Ğ»Ñ similarity search

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹**:
1. **Read from Kafka**: Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ `capsules` topic
2. **Batch aggregation**: Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ 32 capsule
3. **Embedding generation**:
   - Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ journey
   - Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Sentence Transformer model
   - ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ 384-dim vector
4. **Write to Milvus**: bulk insert Ğ² collection
5. **Acknowledgement**: commit Kafka offset

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- Model: `sentence-transformers/all-MiniLM-L6-v2`
- Embedding dimension: 384
- Batch size: 32 Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ throughput
- gRPC client Ğ´Ğ»Ñ Milvus

**Ğ ĞµÑÑƒÑ€ÑÑ‹**:
```yaml
TaskManager:
  memory: 4Gi
  cpu: 2000m
```

---

### 3. Backend Service (Python 3.11 + FastAPI)

#### API Layer
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `backend/src/api/`

**Endpoints**:

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/v1/services` | POST | Register new service |
| `/api/v1/services` | GET | List registered services |
| `/api/v1/analyze` | POST | Start analysis job |
| `/api/v1/analyze/{job_id}` | GET | Get analysis status |
| `/api/v1/slis` | GET | List SLI candidates |
| `/api/v1/slis/{id}/approve` | POST | Approve SLI |
| `/api/v1/slos` | POST | Create SLO |
| `/api/v1/slos/{id}/error-budget` | GET | Current error budget |
| `/api/v1/artifacts` | POST | Generate artifacts |
| `/api/v1/artifacts/{id}/deploy` | POST | Deploy to Git |
| `/health` | GET | Health check |
| `/metrics` | GET | Prometheus metrics |

**Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸**:
- Async I/O Ñ asyncpg (PostgreSQL)
- SQLAlchemy 2.0 ORM
- Pydantic validation
- OpenAPI spec Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ

**Middleware**:
- CORS configuration
- Error handling (structured JSON errors)
- Policy guard (blast radius checking)
- Request validation

---

#### Service Layer
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `backend/src/services/`

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹**:

1. **TraceGraphService** (`trace_graph.py`)
   - ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ¸Ğ· spans
   - DFS/BFS traversal Ğ´Ğ»Ñ entry points
   - Critical path detection

2. **SLIGenerator** (`sli_generator.py`)
   - ĞĞ½Ğ°Ğ»Ğ¸Ğ· capsules Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ SLI candidates
   - Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·: p50, p95, p99 latency
   - Error rate calculation: `(failed_requests / total_requests)`
   - Availability: `(successful_requests / total_requests) * 100`

3. **LLMRecommender** (`llm_recommender.py`)
   - Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ OpenAI GPT-4 / Anthropic Claude
   - Context-aware recommendations
   - Prompt engineering Ğ´Ğ»Ñ SLO generation
   - Retry logic Ñ exponential backoff

4. **ConfidenceScorer** (`confidence_scorer.py`)
   - ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ SLI ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ° (0.0-1.0)
   - Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹:
     - Data completeness (sample size)
     - Statistical significance
     - Journey coverage
     - Error rate variance

5. **PolicyEvaluator** (`policy_evaluator.py`)
   - Blast radius calculation
   - Policy enforcement Ğ¿ĞµÑ€ĞµĞ´ deployment
   - Fail-safe: default deny Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

6. **ArtifactGenerator** (`artifact_generator.py`)
   - Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Prometheus recording rules
   - Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ alert rules Ñ thresholds
   - Grafana dashboard JSON
   - Markdown runbooks

---

#### Workers (Celery 5.3)
**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `backend/src/workers/`

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸**:

1. **journey_discovery.py**
   - Async task Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° traces
   - Timeout: 30 Ğ¼Ğ¸Ğ½ÑƒÑ‚ (configurable)
   - Retry: 3 attempts Ñ exponential backoff

2. **embedding_pipeline.py**
   - Async embedding generation
   - Batch processing Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸

3. **metrics.py**
   - Ğ¡Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ self-observability
   - Publish Ğ² Prometheus pushgateway

**ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Celery**:
```python
worker_pool: prefork
worker_concurrency: 5
task_acks_late: True
task_reject_on_worker_lost: True
soft_time_limit: 1800  # 30 minutes
time_limit: 1900
```

**Queues**:
- `journey_discovery` â€” Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚: high
- `embeddings` â€” Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚: low

---

### 4. Storage Layer

#### PostgreSQL 14 + TimescaleDB

**Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹**:

1. **services**
   - Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
   - ĞŸĞ¾Ğ»Ñ: `id`, `name`, `environment`, `owner_team`, `telemetry_endpoints`, `label_mappings`, `status`
   - Index: `(name, environment)` â€” UNIQUE

2. **telemetry_events** (Hypertable Ğ² TimescaleDB)
   - Time-series Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹
   - ĞŸĞ¾Ğ»Ñ: `event_id`, `service_id`, `timestamp`, `event_type`, `severity`, `trace_id`, `span_id`, `attributes`
   - Partitioning: Ğ¿Ğ¾ `timestamp` Ñ chunk interval 1 Ğ´ĞµĞ½ÑŒ
   - Retention policy: 7 Ğ´Ğ½ĞµĞ¹ (Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°)
   - Indexes:
     - `(service_id, timestamp DESC)` â€” Ğ´Ğ»Ñ queries Ğ¿Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑÑƒ
     - `(trace_id)` â€” Ğ´Ğ»Ñ trace reconstruction
     - `(event_type, timestamp DESC)` â€” Ğ´Ğ»Ñ type-specific queries

3. **capsules** (Hypertable)
   - ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ user journeys
   - ĞŸĞ¾Ğ»Ñ: `capsule_id`, `journey_id`, `service_id`, `start_time`, `end_time`, `fingerprint_hash`, `step_count`, `event_ids`
   - Partitioning: Ğ¿Ğ¾ `start_time` Ñ chunk interval 1 Ñ‡Ğ°Ñ
   - Index: `(fingerprint_hash)` â€” Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… journeys

4. **user_journeys**
   - ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ user journeys
   - ĞŸĞ¾Ğ»Ñ: `journey_id`, `service_id`, `name`, `description`, `entry_points` (JSONB), `critical_path` (JSONB)

5. **slis**
   - SLI ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ñ‹ Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ½Ñ‹Ğµ SLI
   - ĞŸĞ¾Ğ»Ñ: `sli_id`, `journey_id`, `name`, `metric_query`, `threshold_ms`, `error_budget`, `confidence_score`, `approved`, `approved_by`, `approved_at`
   - Index: `(approved)` â€” Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸

6. **slos**
   - SLO Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ
   - ĞŸĞ¾Ğ»Ñ: `slo_id`, `sli_id`, `service_id`, `name`, `target_percentage`, `time_window_days`, `threshold_variant`
   - CHECK constraint: `target_percentage BETWEEN 0 AND 1`

7. **artifacts**
   - Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹
   - ĞŸĞ¾Ğ»Ñ: `artifact_id`, `slo_id`, `artifact_type`, `content` (TEXT), `status`, `pr_url`, `deployed_at`
   - Types: `prometheus_rule`, `grafana_dashboard`, `runbook`

8. **policies**
   - Deployment policies
   - ĞŸĞ¾Ğ»Ñ: `policy_id`, `name`, `blast_radius_threshold`, `enabled`
   - Default policies:
     - Conservative: 10% blast radius
     - Critical Services: 5% blast radius

**Ğ¡Ñ…ĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…**: ÑĞ¼. `backend/migrations/versions/001_initial_schema.py`

---

#### Milvus 2.3+ (Vector Database)

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- Semantic search Ğ¿Ğ¾ embeddings
- Similarity matching Ğ´Ğ»Ñ journey patterns
- K-nearest neighbors (KNN) queries

**Collections**:
- `journey_embeddings` â€” 384-dim vectors (Sentence Transformers)

**Indexes**:
- HNSW (Hierarchical Navigable Small World) Ğ´Ğ»Ñ fast ANN search
- Parameters:
  - `M`: 16 (graph connectivity)
  - `efConstruction`: 200

**Query Example**:
```python
collection.search(
    data=[query_vector],
    anns_field="embedding",
    param={"metric_type": "COSINE", "params": {"ef": 64}},
    limit=10
)
```

---

#### Redis 7.2+

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
1. **Celery backend**: task results storage
2. **Rate limiting**: Ğ´Ğ»Ñ API endpoints
3. **Cache**: frequently accessed data

**ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ**:
- Persistence: AOF (Append-Only File)
- Max memory: 2GB
- Eviction policy: `allkeys-lru`

---

#### MinIO / S3

**ĞĞ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ**:
- Cold storage Ğ´Ğ»Ñ raw traces
- Artifact storage (dashboards, runbooks)
- Backup retention

**Buckets**:
- `raw-traces`: raw OTLP traces (retention 30 days)
- `artifacts`: generated Prometheus rules, dashboards
- `backups`: database backups

---

### 5. Frontend (React 18 + TypeScript)

**Ğ¤Ğ°Ğ¹Ğ»Ñ‹**: `frontend/src/`

**ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹**:
- **ServicesPage** â€” Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ°Ğ¼Ğ¸
- **AnalysisPage** â€” Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ analysis jobs
- **SLIApprovalCard** â€” approval workflow Ğ´Ğ»Ñ SLI
- **SLOsPage** â€” Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ SLO Ğ¸ error budgets
- **ArtifactViewer** â€” Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²

**Tech Stack**:
- React 18 (hooks)
- TypeScript 5.x
- Tailwind CSS
- Fetch API Ğ´Ğ»Ñ backend communication

**Deployment**:
- Nginx static file serving
- NodePort 30080 (Kubernetes)

---

## ĞŸĞ¾Ñ‚Ğ¾Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…

### End-to-End Flow

```
1. Telemetry Sources â†’ Collectors
   - Prometheus scrapes metrics
   - OTLP receivers accept traces
   - Log collectors tail Kubernetes logs
   - Collectors publish to Kafka "raw-telemetry" topic

2. Kafka â†’ Flink Fingerprint Job
   - Consumes "raw-telemetry" topic
   - Groups by trace_id
   - Constructs trace graph
   - Generates fingerprint hash
   - Detects user journeys
   - Publishes "capsules" to Kafka

3. Flink â†’ PostgreSQL
   - Writes capsules to TimescaleDB
   - Hypertable partitioning by timestamp
   - Indexed by fingerprint_hash

4. Kafka â†’ Flink Embedding Job
   - Consumes "capsules" topic
   - Batch aggregation (32 capsules)
   - Generates 384-dim embeddings
   - Writes to Milvus vector DB

5. User â†’ Backend API
   - POST /api/v1/analyze
   - Backend queries PostgreSQL for capsules
   - Queries Milvus for similar journeys
   - Calls TraceGraphService
   - Calls SLIGenerator
   - Calls LLMRecommender (GPT-4)
   - Stores SLI candidates in PostgreSQL

6. User Approval
   - Frontend: SLI approval workflow
   - POST /api/v1/slis/{id}/approve
   - Backend: updates slis.approved = true

7. SLO Creation
   - POST /api/v1/slos
   - Backend: validates policy (blast radius)
   - Creates SLO record

8. Artifact Generation
   - POST /api/v1/artifacts
   - ArtifactGenerator creates:
     - Prometheus recording rule
     - Prometheus alert rule
     - Grafana dashboard JSON
     - Markdown runbook
   - Stores in artifacts table

9. Deployment
   - POST /api/v1/artifacts/{id}/deploy
   - PRGenerator creates GitHub PR
   - Policy guard checks blast radius
   - PR merged â†’ GitOps applies changes
```

---

## ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ

### Ğ“Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | Scaling Strategy | Target Metrics |
|-----------|------------------|----------------|
| **OTLP Collector** | HPA (3-20 pods) | CPU 70% |
| **Backend** | HPA (2-10 pods) | CPU 70%, Memory 80% |
| **Flink TaskManager** | Manual scaling | Backpressure < 10% |
| **Celery Workers** | HPA (2-20 pods) | Queue depth < 100 |
| **PostgreSQL** | Read replicas | Read latency < 10ms |
| **Redis** | Sentinel (HA) | Memory < 80% |
| **Kafka** | Add brokers | Partition lag < 1000 |

### Capacity Planning

**Baseline (small deployment)**:
- Services: 10-50
- Traces/day: 1M
- Capsules/day: 50K
- Storage: 50GB PostgreSQL, 10GB Milvus

**Medium deployment**:
- Services: 50-200
- Traces/day: 10M
- Capsules/day: 500K
- Storage: 500GB PostgreSQL, 100GB Milvus

**Large deployment**:
- Services: 200+
- Traces/day: 100M+
- Capsules/day: 5M+
- Storage: 5TB+ PostgreSQL, 1TB Milvus

---

## ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ

### Self-Observability

**Prometheus Metrics**:
- `sloscout_collector_events_total{collector="prometheus|otlp|log"}` â€” ingested events
- `sloscout_flink_capsules_created_total` â€” capsules created
- `sloscout_api_requests_total{endpoint, status}` â€” API traffic
- `sloscout_sli_generation_duration_seconds` â€” SLI generation latency
- `sloscout_policy_evaluations_total{result="allow|deny"}` â€” policy decisions
- `sloscout_error_budget_remaining{slo_id}` â€” current error budget

**Grafana Dashboards**:
- System health overview
- Kafka lag monitoring
- Flink backpressure
- API latency percentiles (p50, p95, p99)

**Alerts**:
- Collector down > 5 minutes
- Kafka partition lag > 10K messages
- Flink job restart > 3 times/hour
- PostgreSQL connection pool exhaustion
- Milvus query latency > 500ms

---

## Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ

### Authentication & Authorization
- API key authentication Ğ´Ğ»Ñ external clients
- RBAC Ğ´Ğ»Ñ internal services
- Kubernetes ServiceAccounts

### Data Security
- PII redaction Ğ² logs Ğ¸ traces
- Encryption at rest: PostgreSQL TDE (optional)
- Encryption in transit: TLS everywhere
- Secrets management: External Secrets Operator

### Policy Enforcement
- Blast radius checking Ğ¿ĞµÑ€ĞµĞ´ deployment
- Approval workflow Ğ´Ğ»Ñ SLI/SLO changes
- Audit log Ğ´Ğ»Ñ Ğ²ÑĞµÑ… deployments

---

## ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹

### Deployment

```bash
# 1. Create namespace
kubectl create namespace slo-scout

# 2. Install Helm chart
helm install slo-scout ./infrastructure/helm/slo-scout \
  --namespace slo-scout \
  --values infrastructure/helm/slo-scout/values-prod.yaml

# 3. Verify
kubectl get pods -n slo-scout
kubectl logs -n slo-scout -l app=slo-scout-backend --tail=100
```

### Database Migrations

```bash
# Run migration manually
kubectl exec -it -n slo-scout slo-scout-backend-0 -- \
  python -m alembic upgrade head

# Verify
kubectl exec -it -n slo-scout slo-scout-timescaledb-0 -- \
  psql -U postgres -d slo_scout -c "\dt"
```

### Backup & Restore

```bash
# Backup PostgreSQL
kubectl exec -n slo-scout slo-scout-timescaledb-0 -- \
  pg_dump -U postgres slo_scout | gzip > backup-$(date +%Y%m%d).sql.gz

# Restore
gunzip -c backup-20251007.sql.gz | \
  kubectl exec -i -n slo-scout slo-scout-timescaledb-0 -- \
  psql -U postgres slo_scout
```

### Troubleshooting

**Collectors not ingesting**:
```bash
# Check logs
kubectl logs -n slo-scout -l app=otlp-collector --tail=200

# Verify Kafka connectivity
kubectl exec -n slo-scout slo-scout-backend-0 -- \
  telnet slo-scout-kafka 9092
```

**Flink job failing**:
```bash
# Check Flink UI
kubectl port-forward -n slo-scout svc/flink-jobmanager 8081:8081

# View logs
kubectl logs -n slo-scout -l app=flink-taskmanager --tail=500
```

**API slow responses**:
```bash
# Check PostgreSQL connections
kubectl exec -n slo-scout slo-scout-timescaledb-0 -- \
  psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# Check Redis
kubectl exec -n slo-scout slo-scout-redis-0 -- redis-cli INFO
```

---

## Roadmap

### Current Status (v1.0)
âœ… Production-ready
âœ… 154/154 tasks completed
âœ… 183+ tests (95%+ coverage)
âœ… Self-observability

### Planned Features (v1.1)
- [ ] Multi-cluster support
- [ ] Custom embeddings models
- [ ] Advanced policy rules
- [ ] SLO forecasting with ML

---

## ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°

**Documentation**: [docs/](docs/)
**Issues**: GitHub Issues
**Slack**: #slo-scout

---

**Generated**: 2025-10-07
**Version**: 1.0.0
**Status**: Production Ready âœ…
